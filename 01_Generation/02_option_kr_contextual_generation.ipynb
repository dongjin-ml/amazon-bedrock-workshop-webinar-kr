{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c500484b-988b-4919-a073-a16f7b13e6bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 한글-Claude-v2 Model: Bedrock with LangChain using a Prompt that includes Context\n",
    "\n",
    "---\n",
    "### 중요\n",
    "- 이 노트북은 Anthropic 의 Claude-v2 모델 접근 가능한 분만 실행 가능합니다. \n",
    "- 접근이 안되시는 분은 노트북의 코드와 결과 만을 확인 하시면 좋겠습니다.\n",
    "- 만일 실행시에는 **\"과금\"** 이 발생이 되는 부분 유념 해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66baff-a351-4437-a706-2f50c03de6af",
   "metadata": {},
   "source": [
    "## 소개\n",
    "\n",
    "이 노트북에서는 고객 지원 엔지니어로부터 받은 고객 서비스 품질에 만족하지 못한 고객에게 이메일 응답을 생성하는 방법을 보여줍니다. 불만 고객으로부터 받은 실제 이메일의 내용을 제공하여 모델에 추가 컨텍스트를 제공합니다.\n",
    "\n",
    "프롬프트의 추가 컨텍스트로 인해 이 노트북의 Anthropic Claude-v2 언어 모델에서 생성된 텍스트는 이전에 제로샷 프롬프트를 통해 생성된 콘텐츠보다 품질과 관련성이 훨씬 뛰어납니다.\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html)은 언어 모델로 구동되는 애플리케이션을 개발하기 위한 프레임워크입니다. 이 프레임워크의 주요 측면을 통해 다양한 구성 요소를 함께 연결하여 고급 사용 사례를 생성함으로써 대규모 언어 모델을 보강할 수 있습니다.\n",
    "\n",
    "이 노트북에서는 LangChain에서 제공하는 Bedrock API를 사용합니다. 이 예에서 사용된 프롬프트는 텍스트 생성 요청에 컨텍스트를 추가하기 위한 사용자 지정 LangChain 프롬프트 템플릿을 생성합니다.\n",
    "\n",
    "**참고:** *이 노트북은 AWS 환경 내부 또는 외부에서 실행할 수 있습니다.*\n",
    "\n",
    "#### 문맥\n",
    "이전 예제 `01_zero_shot_generation.ipynb`에서는 LangChain 프레임워크를 사용하여 Amazon Bedrock API와 통신하는 방법을 살펴보았습니다. 이 노트북에서는 유사한 사용 사례에 대해 LangChain 프레임워크를 활용하기 위해 'PromptTemplates'의 도움으로 좀 더 복잡함을 추가하려고 합니다. 'PrompTemplates'를 사용하면 나중에 정보를 채울 수 있는 일반 셸을 만들고 다양한 시나리오를 기반으로 모델 출력을 얻을 수 있습니다.\n",
    "\n",
    "이 노트북의 일부로 LangChain 프레임워크 내에서 Amazon Bedrock 통합의 사용과 'PromptTemplate'의 도움으로 텍스트를 생성하는 데 사용할 수 있는 방법을 살펴보겠습니다.\n",
    "\n",
    "#### 패턴\n",
    "추가 예제를 제공하지 않고 출력을 생성하기 위해 내부 모델에 대한 작업, 지침 및 입력으로 구성된 입력을 Amazon Bedrock API의 LangChain 구현에 제공하기만 하면 됩니다. 여기의 목적은 강력한 LLM이 당면한 작업을 쉽게 이해하고 매력적인 출력을 생성하는 방법을 보여주는 것입니다.\n",
    "\n",
    "![](./images/bedrock_langchain.jpg)\n",
    "\n",
    "#### 사용 사례\n",
    "Amazon Bedrock에서 모델의 생성 기능을 시연하기 위해 이메일 생성 사용 사례를 살펴보겠습니다.\n",
    "\n",
    "#### 페르소나\n",
    "귀하는 AnyCompany의 고객 서비스 관리자인 권율이며 일부 고객은 고객 서비스에 만족하지 않고 고객 지원 엔지니어가 제공하는 서비스에 대해 부정적인 피드백을 제공하고 있습니다. 이제는 열악한 서비스에 대해 겸허히 사과하는 고객에게 응답하고 신뢰를 회복하고 싶습니다. 인간 친화적이고 이전 이메일 서신에서 고객의 정서에 맞게 개인화된 대량의 이메일을 생성하려면 LLM의 도움이 필요합니다.\n",
    "\n",
    "#### 구현\n",
    "이 사용 사례를 이행하기 위해 고객의 이전 이메일을 기반으로 감사 메모가 포함된 이메일을 생성하는 방법을 보여줍니다. Amazon Bedrock LangChain 통합을 사용하는 Amazon Titan Text Large 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adadea5-e26a-4280-b988-c9ffb0ae6f4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558a9372-0789-414a-a1d7-2976056f2015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "# os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0dfbfe-182d-4ad2-99a8-30cab8bfd4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                            0.0.249\n",
      "opensearch-py                        2.3.2\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep langchain\n",
    "! pip list | grep opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1fe93c-50e7-4937-9bdd-a5944279b701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import print_ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05ea71b-9c39-4e39-9331-f4892be51010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "\u001b[32m\n",
      "== FM lists ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-G1': 'TBD'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock, print_ww\n",
    "from utils.bedrock import bedrock_info\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "# os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print(colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint(bedrock_info.get_list_fm_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052f263-3a63-4e34-a88c-e5e2e2c278c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. LangChain 통합을 사용하여 Bedrock 클라이언트 호출\n",
    "\n",
    "langchain.llms 로 부터 Bedrock 클래스의 인스턴스를 만드는 것으로 시작하겠습니다. 이것은 Amazon Bedrock에서 사용할 수 있는 모델의 'model_id'를 예상합니다.\n",
    "\n",
    "선택적으로 이전에 생성된 boto3 `client`와 `temperature`, `topP`, `maxTokenCount` 또는 `stopSequences`와 같은 매개변수를 보유할 수 있는 일부 `model_kwargs`를 전달할 수 있습니다(매개변수에 대한 자세한 내용은 Amazon Bedrock에서 탐색할 수 있음)\n",
    "\n",
    "\n",
    "다른 모델은 다른 `model_kwargs`를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffa1250-56cd-4b6d-b3d8-c62baac143ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v1\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2678ed-f0d6-444f-9a57-5170dd1952f7",
   "metadata": {},
   "source": [
    "## 3. LangChain 사용자 정의 프롬프트 템플릿 생성\n",
    "\n",
    "프롬프트에 대한 템플릿을 생성하여 실행할 때마다 다른 입력 변수를 전달할 수 있습니다. 이는 데이터베이스에서 가져올 수 있는 다른 입력 변수를 사용하여 콘텐츠를 생성해야 할 때 유용합니다.\n",
    "\n",
    "이전에는 프롬프트를 하드코딩했습니다. 유사한 부정적인 피드백을 보내는 여러 고객이 있고 이제 각 고객의 이메일을 사용하고 사과로 응답하지만 응답을 약간 개인화하려는 경우가 있을 수 있습니다. 다음 셀에서는 이 패턴을 달성하기 위해 'PromptTemplate'을 만드는 방법을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c4412b-5bb6-4890-8a4a-193c82df7f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"customerServiceManager\", \"customerName\", \"feedbackFromCustomer\"], \n",
    "    template=\"\"\"서비스 관리자 {customerServiceManager}가 {customerName}에게 보내는 사과 이메일을 작성합니다.\n",
    "   고객으로부터 받은 다음 피드백에 대한 응답: {feedbackFromCustomer}.\n",
    "   \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(customerServiceManager=\"권율\", \n",
    "                                 customerName=\"이순신\", \n",
    "                                 feedbackFromCustomer=\"\"\"안녕하세요 권율님,\n",
    "     귀하의 고객 지원팀에 전화했을 때의 최근 경험에 매우 실망했습니다.\n",
    "     나는 즉시 전화를 받을 것으로 예상했지만 전화를 받는 데 3일이 걸렸습니다.\n",
    "     문제를 해결하기 위한 첫 번째 제안이 올바르지 않았습니다. 결국 문제는 3일 만에 해결 되었습니다.\n",
    "     우리는 제공된 응답에 매우 만족하지 않으며 다른 곳에서 비즈니스를 수행하는 것을 고려할 수 있습니다.\n",
    "     \"\"\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb11d864-788d-4063-af44-7842f634c54f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      " 서비스 관리자 권율가 이순신에게 보내는 사과 이메일을 작성합니다.\n",
      "   고객으로부터 받은 다음 피드백에 대한 응답: 안녕하세요 권율님,\n",
      "     귀하의 고객 지원팀에 전화했을 때의 최근 경험에 매우 실망했습니다.\n",
      "     나는 즉시 전화를 받을 것으로 예상했지만 전화를 받는 데 3일이 걸렸습니다.\n",
      "     문제를 해결하기 위한 첫 번째 제안이 올바르지 않았습니다. 결국 문제는 3일 만에 해결 되었습니다.\n",
      "     우리는 제공된 응답에 매우 만족하지 않으며 다른 곳에서 비즈니스를 수행하는 것을 고려할 수 있습니다.\n",
      "     .\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "print(\"prompt: \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232ee0d5-7b80-413f-9e96-f465ecb0c0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HUMAN = \"\\n\\nHuman:\"\n",
    "ASSISTANT = \"\\n\\nAssistant:\"\n",
    "prompt = HUMAN + prompt + ASSISTANT\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45bdfd5-ce76-42e9-81cd-b0892337d163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prompt has 605 tokens\n"
     ]
    }
   ],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf31e9-56c0-408f-a652-9e23de446aef",
   "metadata": {},
   "source": [
    "## 4.프로프트 실행\n",
    "프롬프트 템플릿을 사용하여 호출하고 선별된 응답을 다시 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1064c57-27a4-48c5-911b-e4f1dfeff122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "고객님으로부터 받은 피드백에 대해 진심으로 사과드립니다. 고객님의 경험은 우리의 기대치를 충족시키지 못했고, 이는 우리 서비스의 품질을 떨어뜨릴 수 있습니다.\n",
      "\n",
      "고객님의 문제를 해결하는 데 3일이 소요된 것은 용납할 수 없습니다. 우리의 대응 속도를 높이고 고객 응대 프로세스를 개선해야 할 것입니다. 또한 첫 번째 제안이 올바르지 않았다는\n",
      "점에서 우리의 문제 해결 능력을 개선해야 한다는 것을 알고 있습니다.\n",
      "\n",
      "고객님의 지속적인 지지와 사업을 위해 노력하겠습니다. 우리 서비스의 품질과 고객 경험을 개선하기 위해 최선을 다할 것을 약속 드립니다. 고객님의 소중한 의견이 우리 성장의 동력이\n",
      "될 것입니다.\n",
      "\n",
      "고객님의 지속적인 서포트에 진심으로 감사드립니다. 좋은 하루 되세요!\n",
      "\n",
      "바쁘시겠지만, 추가적인 피드백이 있으시면 언제든 연락주시기 바랍니다.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]\n",
    "\n",
    "print_ww(colored(email, \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9abc40",
   "metadata": {},
   "source": [
    "## 5. 요약\n",
    "\n",
    "결론적으로 우리는 컨텍스트 없이 LLM을 호출하면 원하는 결과를 얻지 못할 수 있음을 배웠습니다. 컨텍스트를 추가하고 프롬프트 템플릿을 추가로 사용하여 LLM의 출력을 제한함으로써 원하는 출력을 성공적으로 얻을 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
